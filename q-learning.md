# Q Learning 기법 소개 

그렇다면 강화 학습을 프로그램으로 구현하기 위해서는 어떻게 해야 할까요? 위의 실습에서 Open AI Gym 프로즌 레이크 게임을 예로 들어 보겠습니다. 위의 게임에서는 현재 상태와 맵 정보, 골은 어디에 있고 어디가 함정인지 모두 알고 있는 상태에서 시작했습니다. 

하지만 실제 게임에서는 전혀 다른 이야기가 됩니다. 이유는 현재 맵 정보를 알지 못하기 때문입니다. 현재 상태에서 어떤 액션을 취했을때 그곳이 보상을 받을수 있는 골 지점인지, 또는 그냥 길인지, 아니면 게임을 종료시키는 함정인지 알 방법이 없습니다. 

여기서 Q가 등장합니다. 위에서 강화학습은 주어진 환경, 상태에서 액션을 취해 환경을 변화시키고 그것에 대한 보상으로 학습을 한다고 했습니다. 만일 이미 상태에 따라 여러가지 액션을 취해보고 그에 따른 결과(보상)를 알고 있는 존재가 있다면 어떨까요?

만약 그런 존재가 있다면 쉽게 목적지까지 도달 할 수 있을 것입니다. Q 라는 존재에게 현재 상태와 액션을 알려주고, 액션 중 가장 보상이 큰 행동을 취하면 될 것이기 때문입니다. 

위에서 설명한 것처럼 Q는 현재상태에서 어떤 액션을 취했을때 보상이 어느정도 되는지 돌려주는 존재입니다. 즉 Q 란 다음과 같은 기능을 제공한다고 볼 수 있습니다.

* 현재 상태에서 특정 액션을 취했을때 기대되는 보상을 알려주는 기능 -> maxQ(상태, 액션)

* 현재 상태에서 가장 보상이 높은 액션을 알려주는 기능 -> argmaxQ(상태, 액션[])

즉 Q 란 존재는 상태와 상태에서 액션을 취했때의 보상 기대값을 저장하고 있는 존재입니다. 그림으로 보면 다음과 같습니다.

<img src="http://postfiles15.naver.net/MjAxNzAyMTdfMTY4/MDAxNDg3MzE5MDkzNTg1.ZlG7qdOAM3U3njCjZIpEm0Si7hTFzR0Wkk3TGRoYGtQg.SkvMMDHur3Yp4uN6Dm0kyKMI_ba6rTcllpjJtaCzjykg.PNG.akj61300/q.png?type=w2" width="600px"/>  

위 그림은 Frozen Lake 게임에서 Q 테이블을 그려 본 것입니다. 각 상태마다 4가지 액션이 있을 수 있고 해당 액션에 대한 기대 보상을 저장합니다. 위와 같이 각 상태마다 액션에 대한 정보를 가지고 있는 것을 Q 테이블이라 부릅니다.

여기서 문제가 하나 있습니다. 처음 게임이 시작 될때 Q 라는 존재 역시 아직 액션에 대한 기대보상을 전혀 모른다는 것이죠. 

즉 최초에는 Q 테이블의 모든 값을 일단 0 으로 초기화하고 액션을 취해가면서 Q 테이블을 업데이트 해나가야 합니다. Q 러닝은 결국 다음과 같은 방식으로 개발된다고 볼수 있습니다.

