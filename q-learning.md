#Q Learning 기법 소개

그렇다면 강화 학습을 프로그램으로 구현하기 위해서는 어떻게 해야 할까요? 위의 실습에서 Open AI Gym 프로즌 레이크 게임을 예로 들어 보겠습니다. 위의 게임에서는 현재 상태와 맵 정보, 골은 어디에 있고 어디가 함정인지 모두 알고 있는 상태에서 시작했습니다.

하지만 실제 게임에서는 전혀 다른 이야기가 됩니다. 이유는 현재 맵 정보를 알지 못하기 때문입니다. 현재 상태에서 어떤 액션을 취했을때 그곳이 보상을 받을수 있는 골 지점인지, 또는 그냥 길인지, 아니면 게임을 종료시키는 함정인지 알 방법이 없습니다.

여기서 Q가 등장합니다. 위에서 강화학습은 주어진 환경, 상태에서 액션을 취해 환경을 변화시키고 그것에 대한 보상으로 학습을 한다고 했습니다. 만일 이미 상태에 따라 여러가지 액션을 취해보고 그에 따른 결과(보상)를 알고 있는 존재가 있다면 어떨까요?

만약 그런 존재가 있다면 쉽게 목적지까지 도달 할 수 있을 것입니다. Q 라는 존재에게 현재 상태와 액션을 알려주고, 액션 중 가장 보상이 큰 행동을 취하면 될 것이기 때문입니다.

위에서 설명한 것처럼 Q는 현재상태에서 어떤 액션을 취했을때 보상이 어느정도 되는지 돌려주는 존재입니다. 즉 Q 란 다음과 같은 기능을 제공한다고 볼 수 있습니다.

* 현재 상태에서 특정 액션을 취했을때 기대되는 보상을 알려주는 기능 -> maxQ(상태, 액션)

* 현재 상태에서 가장 보상이 높은 액션을 알려주는 기능 -> argmaxQ(상태, 액션[])



Frozen Lake 게임을 예로 들어보겠습니다.

위 실습에서 게임을 진행해본 Frozen Lake 게임은 16개의 상태가 있습니다. 각 상태를 테이블처럼 표현하면 다음과 같이 될 것입니다.

<img src="http://postfiles14.naver.net/MjAxNzAyMThfMTg4/MDAxNDg3NDAzMDA3Nzc0.nQedecAA5-pH98E_ndo3XWb6AhqbrGYoMqS8z9MiQxgg.FjYBlLdIgOEiduV6-qpGSAKJuq3LKT1wi-YctEpqLnYg.PNG.akj61300/map.png?type=w2" />  

강화학습을 위해 목적지에 도달한 경우 보상 1을 얻고 그렇지 않은 경우에는 보상이 0 이라고 생각해 보겠습니다. 위에서 Q 는 각 상태에서 취할 수 있는 액션에 대한 기대 보상을 가지고 있다고 했습니다. 그것을 그림으로 표현하면 다음과 같이 될 것입니다.






위 그림은 Frozen Lake 게임에서 Q 테이블을 그려 본 것입니다. 각 상태마다 4가지 액션이 있을 수 있고 해당 액션에 대한 기대 보상을 저장합니다. 위와 같이 각 상태마다 액션에 대한 정보를 가지고 있는 것을 Q 테이블이라 부릅니다.

여기서 문제가 하나 있습니다. 처음 게임이 시작 될때 Q 라는 존재 역시 아직 액션에 대한 기대보상을 전혀 모른다는 것이죠.

즉 최초에는 Q 테이블의 모든 값을 일단 0 으로 초기화하고 액션을 취해가면서 Q 테이블을 업데이트 해나가야 합니다. 액션 역시 아직 Q가 정보를 줄 수 없기 때문에 일단은 아무거나 골라야 합니다. 즉 랜덤하게 선택하여 진행을 합니다.

이렇게 랜덤하게 진행하다 보면 어느 순간 G 지점에 이전 칸에 도달할 때가 있을 것입니다.