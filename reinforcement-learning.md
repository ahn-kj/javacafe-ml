# 인공지능, 머신 러닝, 강화 학습

지난 해 구글 딥 마인드에서 개발한 인공지능 바둑 프로그램 알파고가 세계 정상급 바둑 프로기사인 이세돌 9단을 이기면서 인공지능에 대한 관심은 점차 높아져 가고 있습니다.

알파고 등 최근 인공지능 기술에서 뺄수 없는 큰 가지중 하나가 머신러닝 입니다. 머신러닝은 한국말로 기계학습, 간단히 기계가 학습을 하는것 이라고 할 수 있습니다. 대체 기계가 학습을 한다는게 어떤 것일까요?

<img src="http://postfiles8.naver.net/MjAxNzAyMTdfMTE1/MDAxNDg3MzEzNDczODcw.AJBfGCBNXh7f1gR3Q8cio-UNkhGwfpcIhD3xAocclR0g.L-lhAgLtKKIA9WhQlboN2-et_aJLg46KJprr4TYwHnwg.JPEG.akj61300/ml-sample01.jpg?type=w2" width="800px" />

기계가 학습을 한다고 말하면 위 사진 처럼 로봇이 책을 읽거나 공부를 하는 모습을 상상 할지도 모릅니다. 위의 사진도 틀린 얘기는 아니지만 기계학습의 핵심은 로봇이 아닌 스스로 학습을 하는 소프트웨어를 이야기합니다.

현존하는 대부분의 소프트웨어는 프로그래머 또는 개발자라고 불리우는 사람들의 손에서 탄생됩니다. 간단한 프로그램이라도 특정 조건에 따라 프로그램이 어떻게 동작해야 하는지 사람이 일일이 프로그래밍 하는 것이죠. 심하게 말하면 컴퓨터 또는 프로그램이 자동으로 되는 것은 아무것도 없다고 볼수 있습니다. 모두 사람 손에 의해 만들어졌고, 의도한대로 동작되는 것입니다.(의도한대로 동작 안될수도 있지만 그런것은 보통 버그라고 부릅니다.) 

여기서 머신러닝 기반 소프트웨어와 그렇지 않은 소프트웨어의 아이덴티티가 결정됩니다. 일반 소프트웨어는 동작방식을 사람이 일일이 설계하는 것이고, 머신러닝 소프트웨어는 사람이 설계하지 않아도 동작방식을 스스로 바꿀수 있다는 것이죠.

어떻게 이런일이 가능할까요?

머신러닝 소프트웨어를 만들기 위한 방법으로는 크게 3가지 방법을 보통 이야기합니다. 지도학습, 비지도학습, 강화학습이 그것이죠. 각 방법을 간략하게 소개하면 다음과 같습니다.

* 지도학습: 사람에 입력 데이터와 정답을 제공하여 학습 시키는 방법. 예를 들어 이미지(입력)와 이미지에 해당하는 정답(사람, 개, 고양이) 등을 제공하고 학습뒤 이미지(입력)이 어떤 정보인지 추측하는 프로그램
 
* 비지도학습: 지도학습과 달리 컴퓨터가 정답이 없는 정보를 적절히 군으로 분류하여 학습하는 방법.

* 강화학습: 주어진 데이터가 아닌, 주어진 환경에서 행위를 반복하며 환경변화를 만들어 학습하는 방법.

이 컨텐츠에서 다룰 내용은 강화학습에 대한 내용입니다. 작년 화제가 되었던 알파고는 여러가지 알고리즘들이 복합되었지만 가장 중요한 부분중 하나가 바로 이 강화학습이라고 볼 수 있습니다.

여기서는 강화학습에 대한 개념들을 정리하고, 개념 이해를 돕기 위해 텐서플로우와 Open AI Gym 을 사용한 실습을 소개하려 합니다.

또 이 컨텐츠는 아래 [모두를 위한 딥러닝 - 시즌RL 편](http://hunkim.github.io/ml/) 을 본뒤 기타 다른 자료들을 공부하여 작성한 것입니다. 해당 링크에 동영상 강좌 역시 참조하시면 좋다고 생각됩니다.

# 강화학습

> 강화학습의 기본 개념, 동작원리, 뭘하고 싶은건지 등등


# 실습을 위한 환경 준비

> TODO: 하단 실습을 위한 환경을 정리하는 단락입니다.

# Open AI Gym 의 개념

> TODO: 왜 OPEN AI Gym 을 사용하는지, OPEN AI Gym 에 대한 설명

# Open AI Gym 실습

> TODO: 실제 실습 코드와 쉬운 주석, 코드 동작 원리등을 정리하면 될것 같습니다.

# Q Learning 기법 소개 

그렇다면 강화 학습을 프로그램으로 구현하기 위해서는 어떻게 해야 할까요? 위의 실습에서 Open AI Gym 프로즌 레이크 게임을 예로 들어 보겠습니다. 위의 게임에서는 현재 상태와 맵 정보, 골은 어디에 있고 어디가 함정인지 모두 알고 있는 상태에서 시작했습니다. 

하지만 실제 게임에서는 전혀 다른 이야기가 됩니다. 이유는 현재 맵 정보를 알지 못하기 때문입니다. 현재 상태에서 어떤 액션을 취했을때 그곳이 보상을 받을수 있는 골 지점인지, 또는 그냥 길인지, 아니면 게임을 종료시키는 함정인지 알 방법이 없습니다. 

여기서 Q가 등장합니다. 위에서 강화학습은 주어진 환경, 상태에서 액션을 취해 환경을 변화시키고 그것에 대한 보상으로 학습을 한다고 했습니다. 만일 이미 상태에 따라 여러가지 액션을 취해보고 그에 따른 결과(보상)를 알고 있는 존재가 있다면 어떨까요?

만약 그런 존재가 있다면 쉽게 목적지까지 도달 할 수 있을 것입니다. Q 라는 존재에게 현재 상태와 액션을 알려주고, 액션 중 가장 보상이 큰 행동을 취하면 될 것이기 때문입니다. 

위에서 설명한 것처럼 Q는 현재상태에서 어떤 액션을 취했을때 보상이 어느정도 되는지 돌려주는 존재입니다. 즉 Q 란 다음과 같은 기능을 제공한다고 볼 수 있습니다.

* 현재 상태에서 특정 액션을 취했을때 기대되는 보상을 알려주는 기능 -> maxQ(상태, 액션)

* 현재 상태에서 가장 보상이 높은 액션을 알려주는 기능 -> argmaxQ(상태, 액션[])

즉 Q 란 존재는 상태와 상태에서 액션을 취했때의 보상 기대값을 저장하고 있는 존재입니다. 그림으로 보면 다음과 같습니다.

<img src="http://postfiles15.naver.net/MjAxNzAyMTdfMTY4/MDAxNDg3MzE5MDkzNTg1.ZlG7qdOAM3U3njCjZIpEm0Si7hTFzR0Wkk3TGRoYGtQg.SkvMMDHur3Yp4uN6Dm0kyKMI_ba6rTcllpjJtaCzjykg.PNG.akj61300/q.png?type=w2" width="600px"/>  

위 그림은 Frozen Lake 게임에서 Q 테이블을 그려 본 것입니다. 각 상태마다 4가지 액션이 있을 수 있고 해당 액션에 대한 기대 보상을 저장합니다. 위와 같이 각 상태마다 액션에 대한 정보를 가지고 있는 것을 Q 테이블이라 부릅니다.

여기서 문제가 하나 있습니다. 처음 게임이 시작 될때 Q 라는 존재 역시 아직 액션에 대한 기대보상을 전혀 모른다는 것이죠. 

즉 최초에는 Q 테이블의 모든 값을 일단 0 으로 초기화하고 액션을 취해가면서 Q 테이블을 업데이트 해나가야 합니다. Q 러닝은 결국 다음과 같은 방식으로 개발된다고 볼수 있습니다.

 

# 간단한 Q 알고리즘 실습  

> TODO: 제 실습 코드와 쉬운 주석, 코드 동작 원리등을 정리하면 될것 같습니다
 
